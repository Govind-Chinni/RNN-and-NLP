1. What is the main architectural difference between BERT and GPT?
Ans:
The original Transformer has two main parts:
	Encoder: Processes the entire input all at once.
	Decoder: Generates output step by step (like in translation or text generation).

* BERT uses only the encoder part of the Transformer architecture.
* GPT uses only the decoder part of the Transformer.
* BERT is bidirectional, meaning it looks at the entire sentence at once to understand context.
* GPT is unidirectional (left-to-right), meaning it generates text one word at a time, based on previous words.
* BERT is designed for understanding tasks like classification, NER, and QA.
* GPT is designed for generative tasks like text completion, summarization, and dialogue.


2. Why is using pre-trained models like BERT or GPT beneficial instead of training from scratch?
Ans: 	Pretrained models are trained on massive text corpora and learn general language features.
	They save time and resources—you don’t have to train from scratch.
	They require much less labeled data for your specific task (thanks to transfer learning).
	They are more accurate, especially on small datasets, because they already understand grammar and language structure.
	Using them helps avoid the need for powerful GPUs/TPUs and weeks of training time.
	They are widely supported and regularly updated by the research community.

